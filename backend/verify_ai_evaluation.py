#!/usr/bin/env python3
"""
è‡ªåŠ¨åŒ–éªŒè¯å¹¶ç”Ÿæˆæ‰€æœ‰AIè¯„æµ‹é›†å†…å®¹

ä¾æ¬¡ç”Ÿæˆæ¯ä¸ªAIä¸»é¢˜ï¼Œå¦‚æœå¤±è´¥åˆ™åœæ­¢å¹¶æŠ¥å‘Šé”™è¯¯ã€‚
æˆåŠŸåè‡ªåŠ¨å¤åˆ¶åˆ°å‰ç«¯é¡µé¢ç›®å½•ã€‚
"""

import os
import sys
import time
import json
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

# Add backend to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from models.schemas import (
    KnowledgePath,
    KnowledgePoint,
    CognitiveLevel,
    GenerationRequest,
    DifficultyLevel
)
from workflows.pipeline import create_pipeline
from agents.assembler import AssemblerAgent
from evaluation_set import EVALUATION_SETS


def convert_evaluation_set_to_request(eval_set):
    """å°†è¯„æµ‹é›†è½¬æ¢ä¸ºç”Ÿæˆè¯·æ±‚"""
    knowledge_points = []

    for kp_data in eval_set["knowledge_points"]:
        kp = KnowledgePoint(
            knowledge_id=kp_data["knowledge_id"],
            name=kp_data["name"],
            description=kp_data["description"],
            domain=kp_data["domain"],
            subdomain=kp_data["subdomain"],
            difficulty=kp_data["difficulty"],
            cognitive_level=CognitiveLevel(kp_data["cognitive_level"]),
            importance=kp_data["importance"],
            abstraction=kp_data["abstraction"],
            estimated_time=kp_data["estimated_time"],
            is_key_point=kp_data["is_key_point"],
            is_difficult=kp_data["is_difficult"],
            prerequisites=kp_data["prerequisites"],
            successors=kp_data["successors"],
            keywords=kp_data["keywords"],
            application_scenarios=kp_data["application_scenarios"],
            common_misconceptions=kp_data["common_misconceptions"],
            mastery_criteria=kp_data["mastery_criteria"]
        )
        knowledge_points.append(kp)

    knowledge_path = KnowledgePath(
        knowledge_points=knowledge_points,
        domain=eval_set["domain"],
        target_audience=eval_set["target_audience"]
    )

    request = GenerationRequest(
        knowledge_path=knowledge_path,
        target_audience=eval_set["target_audience"],
        difficulty=DifficultyLevel(eval_set["difficulty"]),
        user_intent=eval_set["user_intent"],
        custom_title=eval_set["topic"],
        page_id=eval_set["set_id"]
    )

    return request


def evaluate_components(generated_schema, expected_components):
    """è¯„ä¼°ç”Ÿæˆçš„ç»„ä»¶"""
    generated_components = set()
    for block in generated_schema.components:
        generated_components.add(block.type.value)

    expected = set(expected_components)
    matched = generated_components & expected
    coverage = len(matched) / len(expected) if expected else 0

    # è®¡ç®—ç¼ºå¤±å’Œæ„å¤–çš„ç»„ä»¶
    missing = expected - generated_components
    unexpected = generated_components - expected

    return {
        "expected": list(expected),
        "generated": list(generated_components),
        "matched": list(matched),
        "missing": list(missing),
        "unexpected": list(unexpected),
        "coverage": coverage
    }


def main():
    """ä¸»å‡½æ•°ï¼šä¾æ¬¡ç”Ÿæˆæ‰€æœ‰AIä¸»é¢˜"""
    print("\n" + "="*70)
    print("ğŸ¤– AIé€šè¯†è¯„æµ‹é›† - è‡ªåŠ¨åŒ–ç”ŸæˆéªŒè¯")
    print("="*70)

    # æ£€æŸ¥ API key
    api_key = os.getenv("GLM_API_KEY") or os.getenv("LLM_API_KEY") or os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("\nâŒ é”™è¯¯: æœªè®¾ç½® LLM API Key")
        print("\nè¯·è®¾ç½®ä»¥ä¸‹ä¹‹ä¸€:")
        print("  export GLM_API_KEY='your-glm-key'")
        print("  export LLM_API_KEY='your-api-key'")
        print("  export OPENAI_API_KEY='your-openai-key'")
        return

    # åˆ›å»º pipeline
    print("\nğŸ”§ åˆå§‹åŒ– Pipeline...")
    pipeline = create_pipeline()
    print("âœ… Pipeline åˆå§‹åŒ–å®Œæˆ\n")

    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs("evaluation_results", exist_ok=True)
    os.makedirs("../public/pages", exist_ok=True)

    # ç”Ÿæˆè®°å½•
    results = []
    successful_sets = []

    # ä¾æ¬¡ç”Ÿæˆæ¯ä¸ªä¸»é¢˜
    for i, eval_set in enumerate(EVALUATION_SETS, 1):
        print(f"\n{'='*70}")
        print(f"ğŸ¤– [{i}/10] {eval_set['set_id']} - {eval_set['topic']}")
        print(f"{'='*70}")
        print(f"   éš¾åº¦: {eval_set['difficulty']}")
        print(f"   å—ä¼—: {eval_set['target_audience']}")
        print(f"   é¢„æœŸç»„ä»¶: {', '.join(eval_set['expected_components'])}")

        start_time = time.time()

        try:
            # è½¬æ¢ä¸ºè¯·æ±‚
            request = convert_evaluation_set_to_request(eval_set)

            # è¿è¡Œ pipeline
            response = pipeline.run(request)

            elapsed_time = time.time() - start_time

            if response.success:
                # è¯„ä¼°ç»„ä»¶
                component_eval = evaluate_components(
                    response.page_schema,
                    eval_set["expected_components"]
                )

                print(f"\nâœ… ç”ŸæˆæˆåŠŸï¼")
                print(f"   ç”¨æ—¶: {elapsed_time:.1f}ç§’")
                print(f"   Tokens: {response.tokens_used}")
                print(f"   ç« èŠ‚æ•°: {len(response.page_schema.sections)}")
                print(f"   ç»„ä»¶æ•°: {len(response.page_schema.components)}")

                print(f"\nğŸ“Š ç»„ä»¶è¯„ä¼°:")
                print(f"   è¦†ç›–ç‡: {component_eval['coverage']*100:.0f}%")
                print(f"   åŒ¹é…: {', '.join(component_eval['matched']) if component_eval['matched'] else 'æ— '}")

                if component_eval['missing']:
                    print(f"   ç¼ºå¤±: {', '.join(component_eval['missing'])}")
                if component_eval['unexpected']:
                    print(f"   é¢å¤–: {', '.join(component_eval['unexpected'])}")

                # ä¿å­˜åˆ°evaluation_results
                result_path = f"evaluation_results/{eval_set['set_id']}.json"
                assembler = AssemblerAgent()
                assembler.export_to_json(response.page_schema, result_path)
                print(f"   ğŸ’¾ å·²ä¿å­˜: {result_path}")

                # å¤åˆ¶åˆ°å‰ç«¯
                frontend_path = f"../public/pages/{eval_set['set_id']}.json"
                assembler.export_to_json(response.page_schema, frontend_path)
                print(f"   ğŸŒ å‰ç«¯é¡µé¢: {eval_set['set_id']}.json")

                # è®°å½•ç»“æœ
                result = {
                    "set_id": eval_set["set_id"],
                    "topic": eval_set["topic"],
                    "success": True,
                    "generation_time": elapsed_time,
                    "tokens_used": response.tokens_used,
                    "sections": len(response.page_schema.sections),
                    "components": len(response.page_schema.components),
                    "component_coverage": component_eval['coverage'],
                    "generated_at": datetime.now().isoformat()
                }
                results.append(result)
                successful_sets.append(eval_set['set_id'])

            else:
                print(f"\nâŒ ç”Ÿæˆå¤±è´¥: {response.error}")
                print(f"\nâš ï¸  éœ€è¦ä¿®å¤é—®é¢˜åä»å¤´é‡æ–°è¿è¡Œ")

                # ä¿å­˜å¤±è´¥è®°å½•
                result = {
                    "set_id": eval_set["set_id"],
                    "topic": eval_set["topic"],
                    "success": False,
                    "error": response.error,
                    "generated_at": datetime.now().isoformat()
                }
                results.append(result)

                # ä¿å­˜ä¸­é—´ç»“æœ
                with open("evaluation_results/_latest.json", "w", encoding="utf-8") as f:
                    json.dump({
                        "total": i,
                        "successful": len(successful_sets),
                        "results": results
                    }, f, ensure_ascii=False, indent=2)

                print(f"\nğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœåˆ°: evaluation_results/_latest.json")
                print(f"\nâŒ åœæ­¢éªŒè¯æµç¨‹ã€‚è¯·ä¿®å¤é”™è¯¯åé‡æ–°è¿è¡Œã€‚")
                return

        except Exception as e:
            print(f"\nâŒ å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()

            # ä¿å­˜å¤±è´¥è®°å½•
            result = {
                "set_id": eval_set["set_id"],
                "topic": eval_set["topic"],
                "success": False,
                "error": str(e),
                "generated_at": datetime.now().isoformat()
            }
            results.append(result)

            # ä¿å­˜ä¸­é—´ç»“æœ
            with open("evaluation_results/_latest.json", "w", encoding="utf-8") as f:
                json.dump({
                    "total": i,
                    "successful": len(successful_sets),
                    "results": results
                }, f, ensure_ascii=False, indent=2)

            print(f"\nğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœåˆ°: evaluation_results/_latest.json")
            print(f"\nâŒ åœæ­¢éªŒè¯æµç¨‹ã€‚è¯·ä¿®å¤é”™è¯¯åé‡æ–°è¿è¡Œã€‚")
            return

    # å…¨éƒ¨å®Œæˆ
    total_time = sum(r.get("generation_time", 0) for r in results if r.get("success", False))

    print("\n" + "="*70)
    print("ğŸ‰ æ‰€æœ‰AIä¸»é¢˜ç”Ÿæˆå®Œæˆï¼")
    print("="*70)
    print(f"\nâœ… æˆåŠŸç”Ÿæˆ: {len(successful_sets)}/10")
    print(f"â±ï¸  æ€»ç”¨æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")

    # ç»Ÿè®¡ç»„ä»¶è¦†ç›–
    if successful_sets:
        all_coverage = [r.get("component_coverage", 0) for r in results if r.get("success", False)]
        avg_coverage = sum(all_coverage) / len(all_coverage)
        print(f"\nğŸ“Š å¹³å‡ç»„ä»¶è¦†ç›–ç‡: {avg_coverage*100:.1f}%")

    # ä¿å­˜æœ€ç»ˆç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    results_file = f"evaluation_results/ai_all_results_{timestamp}.json"

    with open(results_file, "w", encoding="utf-8") as f:
        json.dump({
            "timestamp": datetime.now().isoformat(),
            "total": len(EVALUATION_SETS),
            "successful": len(successful_sets),
            "failed": len(EVALUATION_SETS) - len(successful_sets),
            "total_time_minutes": total_time / 60,
            "results": results
        }, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

    # æ˜¾ç¤ºé¡µé¢è®¿é—®ä¿¡æ¯
    print("\n" + "="*70)
    print("ğŸŒ å‰ç«¯é¡µé¢è®¿é—®")
    print("="*70)
    print("\næ‰€æœ‰AIä¸»é¢˜é¡µé¢å·²ç”Ÿæˆï¼Œå¯ä»¥åœ¨æµè§ˆå™¨ä¸­è®¿é—®ï¼š\n")

    for eval_set in EVALUATION_SETS:
        if eval_set['set_id'] in successful_sets:
            print(f"  {eval_set['set_id']:12s} - {eval_set['topic']}")

    print(f"\nè®¿é—®åœ°å€æ ¼å¼: http://localhost:8080/#/page/{{set_id}}")
    print(f"\nç¤ºä¾‹:")
    print(f"  http://localhost:8080/#/page/ai_001  - {EVALUATION_SETS[0]['topic']}")
    print(f"  http://localhost:8080/#/page/ai_010  - {EVALUATION_SETS[9]['topic']}")


if __name__ == "__main__":
    main()
