# Multi-Agent Content Generation Pipeline - Environment Variables

# ============ Required ============

# LLM API Key (OpenAI-compatible API)
# Supports: OpenAI, SiliconFlow, GLM (智谱), Azure OpenAI, or any OpenAI-compatible API
LLM_API_KEY=your-api-key-here

# ============ Optional ============

# LLM Provider (default: custom)
# Options: openai, azure, siliconflow, glm, custom
LLM_PROVIDER=custom

# API Base URL (for custom APIs)
# For SiliconFlow: https://api.siliconflow.cn/v1
# For GLM: https://open.bigmodel.cn/api/paas/v4/
# For Azure OpenAI: https://your-resource.openai.azure.com/
LLM_BASE_URL=https://api.siliconflow.cn/v1

# Model name
# SiliconFlow: deepseek-ai/DeepSeek-V3
# GLM: glm-4-flash, glm-4-plus, glm-4
# OpenAI: gpt-4o
LLM_MODEL=deepseek-ai/DeepSeek-V3

# ============ GLM (智谱) Configuration ============

# GLM API Key
# Get your API key from: https://open.bigmodel.cn/
# GLM_API_KEY=your-glm-api-key

# GLM Base URL (default)
# GLM_BASE_URL=https://open.bigmodel.cn/api/paas/v4/

# GLM Model (recommended: glm-4-flash for speed, glm-4-plus for quality)
# GLM_MODEL=glm-4-flash

# Generation parameters
LLM_TEMPERATURE=0.3
LLM_MAX_TOKENS=4096

# ============ Optional: Alternative Configuration ============

# If you prefer to use OpenAI standard environment variables:
# OPENAI_API_KEY=your-openai-key
# OPENAI_BASE_URL=https://api.openai.com/v1

# ============ API Configuration ============

MAX_CONCURRENT_GENERATIONS=5
API_PORT=8000
API_HOST=0.0.0.0

# ============ Optional: Monitoring ============

# Enable Prometheus metrics
# ENABLE_METRICS=true
# METRICS_PORT=9090
