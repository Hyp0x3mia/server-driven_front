#!/usr/bin/env python3
"""
GLM API é›†æˆæµ‹è¯•è„šæœ¬

æµ‹è¯• GLM (æ™ºè°±) API çš„è¿æ¥å’ŒåŸºæœ¬åŠŸèƒ½
"""

import os
import sys
import time
from datetime import datetime

# Add backend to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from dotenv import load_dotenv

load_dotenv()

def print_section(title):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print("\n" + "="*70)
    print(f"  {title}")
    print("="*70)


def test_env_config():
    """æµ‹è¯• 1: ç¯å¢ƒå˜é‡é…ç½®"""
    print_section("æµ‹è¯• 1: ç¯å¢ƒå˜é‡é…ç½®")

    # æ£€æŸ¥ GLM ç›¸å…³ç¯å¢ƒå˜é‡
    glm_key = os.getenv("GLM_API_KEY")
    glm_base = os.getenv("GLM_BASE_URL")
    glm_model = os.getenv("GLM_MODEL")

    print(f"\nğŸ“‹ ç¯å¢ƒå˜é‡æ£€æŸ¥:")
    print(f"   GLM_API_KEY: {'âœ… å·²è®¾ç½®' if glm_key else 'âŒ æœªè®¾ç½®'}")
    print(f"   GLM_BASE_URL: {'âœ… ' + glm_base if glm_base else 'âŒ æœªè®¾ç½®'}")
    print(f"   GLM_MODEL: {'âœ… ' + glm_model if glm_model else 'âŒ æœªè®¾ç½®'}")

    # æ£€æŸ¥é€šç”¨å˜é‡
    llm_key = os.getenv("LLM_API_KEY")
    llm_base = os.getenv("LLM_BASE_URL")
    llm_model = os.getenv("LLM_MODEL")

    print(f"\n   LLM_API_KEY: {'âœ… å·²è®¾ç½®' if llm_key else 'âŒ æœªè®¾ç½®'}")
    print(f"   LLM_BASE_URL: {'âœ… ' + llm_base if llm_base else 'âŒ æœªè®¾ç½®'}")
    print(f"   LLM_MODEL: {'âœ… ' + llm_model if llm_model else 'âŒ æœªè®¾ç½®'}")

    # åˆ¤æ–­æ˜¯å¦æœ‰ API key
    api_key = glm_key or llm_key or os.getenv("OPENAI_API_KEY")
    if not api_key:
        print(f"\nâŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½• API Key")
        print(f"   è¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ä¹‹ä¸€:")
        print(f"   - GLM_API_KEY (æ¨è)")
        print(f"   - LLM_API_KEY")
        print(f"   - OPENAI_API_KEY")
        return False

    print(f"\nâœ… API é…ç½®æ£€æŸ¥é€šè¿‡")
    return True


def test_llm_import():
    """æµ‹è¯• 2: LLM å®¢æˆ·ç«¯å¯¼å…¥"""
    print_section("æµ‹è¯• 2: LLM å®¢æˆ·ç«¯å¯¼å…¥")

    try:
        from llm.client import create_llm_from_env, LLMConfig
        print("   âœ… llm.client å¯¼å…¥æˆåŠŸ")

        from llm.client import create_llm
        print("   âœ… create_llm å‡½æ•°å¯¼å…¥æˆåŠŸ")

        return True
    except ImportError as e:
        print(f"   âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False


def test_llm_connection():
    """æµ‹è¯• 3: LLM è¿æ¥æµ‹è¯•"""
    print_section("æµ‹è¯• 3: LLM è¿æ¥æµ‹è¯•")

    try:
        from llm.client import create_llm_from_env

        print("\nğŸ“¡ æ­£åœ¨è¿æ¥ LLM...")
        llm = create_llm_from_env()

        # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
        print(f"\nğŸ“‹ LLM é…ç½®:")
        print(f"   Model: {llm.model_name}")
        print(f"   Temperature: {llm.temperature}")
        print(f"   Max Tokens: {llm.max_tokens}")
        if hasattr(llm, 'base_url'):
            base_url = llm.base_url or "Default (OpenAI)"
            print(f"   Base URL: {base_url}")

        # æµ‹è¯•è°ƒç”¨
        print(f"\nğŸ”„ æµ‹è¯•ç®€å•è°ƒç”¨...")
        start_time = time.time()

        test_message = "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚"
        response = llm.invoke(test_message)

        elapsed_time = time.time() - start_time

        print(f"   âœ… è°ƒç”¨æˆåŠŸ (è€—æ—¶ {elapsed_time:.2f}s)")
        print(f"\nğŸ“ å“åº”å†…å®¹:")
        print(f"   {response.content[:200]}")

        return True

    except Exception as e:
        print(f"   âŒ è¿æ¥å¤±è´¥: {e}")
        print(f"\nğŸ’¡ å¯èƒ½çš„åŸå› :")
        print(f"   1. API Key é”™è¯¯")
        print(f"   2. Base URL é”™è¯¯")
        print(f"   3. æ¨¡å‹åç§°é”™è¯¯")
        print(f"   4. ç½‘ç»œè¿æ¥é—®é¢˜")
        return False


def test_knowledge_path_creation():
    """æµ‹è¯• 4: çŸ¥è¯†è·¯å¾„åˆ›å»º"""
    print_section("æµ‹è¯• 4: çŸ¥è¯†è·¯å¾„åˆ›å»º")

    try:
        from models.schemas import KnowledgePath, KnowledgePoint, CognitiveLevel

        # åˆ›å»ºæµ‹è¯•çŸ¥è¯†ç‚¹
        kp1 = KnowledgePoint(
            knowledge_id="TEST-001",
            name="è‡ªç„¶è¯­è¨€å¤„ç†",
            description="NLP æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯",
            domain="äººå·¥æ™ºèƒ½",
            subdomain="è‡ªç„¶è¯­è¨€å¤„ç†",
            difficulty=1,
            cognitive_level=CognitiveLevel.COG_L1,
            importance=0.9,
            abstraction=2,
            estimated_time=15,
            is_key_point=True,
            is_difficult=False,
            prerequisites=[],
            successors=[],
            keywords=["NLP", "AI", "æ–‡æœ¬å¤„ç†"],
            application_scenarios=["æœºå™¨ç¿»è¯‘", "æ™ºèƒ½å®¢æœ"],
            common_misconceptions=["NLP åªèƒ½å¤„ç†è‹±æ–‡"],
            mastery_criteria="èƒ½å¤Ÿç†è§£ NLP çš„åŸºæœ¬å®šä¹‰å’Œåº”ç”¨"
        )

        kp2 = KnowledgePoint(
            knowledge_id="TEST-002",
            name="Transformer æ¨¡å‹",
            description="Transformer æ˜¯ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„ç¥ç»ç½‘ç»œæ¶æ„",
            domain="äººå·¥æ™ºèƒ½",
            subdomain="æ·±åº¦å­¦ä¹ ",
            difficulty=2,
            cognitive_level=CognitiveLevel.COG_L2,
            importance=0.8,
            abstraction=3,
            estimated_time=30,
            is_key_point=True,
            is_difficult=True,
            prerequisites=["TEST-001"],
            successors=[],
            keywords=["Transformer", "Attention", "ç¥ç»ç½‘ç»œ"],
            application_scenarios=["æœºå™¨ç¿»è¯‘", "æ–‡æœ¬ç”Ÿæˆ"],
            common_misconceptions=["Transformer åªèƒ½ç”¨äº NLP"],
            mastery_criteria="èƒ½å¤Ÿç†è§£ Transformer çš„åŸºæœ¬åŸç†"
        )

        # åˆ›å»ºçŸ¥è¯†è·¯å¾„
        knowledge_path = KnowledgePath(
            knowledge_points=[kp1, kp2],
            domain="äººå·¥æ™ºèƒ½",
            target_audience="åˆå­¦è€…"
        )

        print(f"\nâœ… çŸ¥è¯†è·¯å¾„åˆ›å»ºæˆåŠŸ")
        print(f"   çŸ¥è¯†ç‚¹æ•°é‡: {len(knowledge_path.knowledge_points)}")
        print(f"   æ€»é¢„è®¡æ—¶é—´: {knowledge_path.get_total_estimated_time()} åˆ†é’Ÿ")
        print(f"   é¢†åŸŸ: {knowledge_path.domain}")

        return True, knowledge_path

    except Exception as e:
        print(f"   âŒ åˆ›å»ºå¤±è´¥: {e}")
        return False, None


def test_narrative_generation(knowledge_path):
    """æµ‹è¯• 5: å™è¿°åŒ–ä¸Šä¸‹æ–‡ç”Ÿæˆ"""
    print_section("æµ‹è¯• 5: å™è¿°åŒ–ä¸Šä¸‹æ–‡ç”Ÿæˆ")

    try:
        from models.adapters import knowledge_path_to_skeleton
        from models.narrative import create_narrative_profile

        # è½¬æ¢ä¸º skeleton
        print(f"\nğŸ”„ è½¬æ¢çŸ¥è¯†è·¯å¾„ä¸º PageSkeleton...")
        skeleton = knowledge_path_to_skeleton(knowledge_path)

        # è®¡ç®—æ€»èŠ‚ç‚¹æ•°
        total_nodes = sum(len(section.nodes) for section in skeleton.sections)

        print(f"   âœ… è½¬æ¢æˆåŠŸ")
        print(f"   Sections æ•°é‡: {len(skeleton.sections)}")
        print(f"   æ€»èŠ‚ç‚¹æ•°: {total_nodes}")

        # ç”Ÿæˆå™è¿°åŒ–æè¿°
        print(f"\nğŸ“ ç”Ÿæˆå™è¿°åŒ–æè¿°...")
        node_count = 0
        for section in skeleton.sections:
            for node in section.nodes:
                if node_count >= 2:  # åªæ˜¾ç¤ºå‰ä¸¤ä¸ª
                    break
                profile = create_narrative_profile(node, style="full")
                print(f"\n--- èŠ‚ç‚¹ {node_count + 1}: {node.title} ---")
                print(profile[:300] + "..." if len(profile) > 300 else profile)
                node_count += 1
            if node_count >= 2:
                break

        return True, skeleton

    except Exception as e:
        print(f"   âŒ å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False, None


def test_planner_agent(skeleton):
    """æµ‹è¯• 6: Planner Agent"""
    print_section("æµ‹è¯• 6: Planner Agent (å¦‚æœéœ€è¦ LLM)")

    try:
        from agents.planner import PlannerAgent

        print(f"\nğŸ¤– åˆ›å»º Planner Agent...")
        planner = PlannerAgent()

        print(f"   âœ… Planner Agent åˆ›å»ºæˆåŠŸ")

        # æ³¨æ„: skeleton å·²ç»é€šè¿‡ adapter ç”Ÿæˆï¼Œä¸éœ€è¦ LLM
        print(f"\nâœ… Planner å¯ä»¥ç›´æ¥ä½¿ç”¨ adapter ç”Ÿæˆçš„ skeleton")
        print(f"   (æ— éœ€è°ƒç”¨ LLM)")

        return True

    except Exception as e:
        print(f"   âŒ å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_simple_content_generation():
    """æµ‹è¯• 7: ç®€å•å†…å®¹ç”Ÿæˆ"""
    print_section("æµ‹è¯• 7: ç®€å•å†…å®¹ç”Ÿæˆ (å®Œæ•´æµç¨‹æµ‹è¯•)")

    try:
        from models.schemas import GenerationRequest
        from workflows.pipeline import create_pipeline

        print(f"\nğŸ¤– åˆ›å»º Pipeline...")
        pipeline = create_pipeline()

        # åˆ›å»ºç®€å•è¯·æ±‚
        request = GenerationRequest(
            topic="æœºå™¨å­¦ä¹ åŸºç¡€",
            target_audience="åˆå­¦è€…",
            user_intent="ç®€å•ä»‹ç»æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ",
            max_sections=3
        )

        print(f"   âœ… è¯·æ±‚åˆ›å»ºæˆåŠŸ")
        print(f"   ä¸»é¢˜: {request.topic}")
        print(f"   ç›®æ ‡å—ä¼—: {request.target_audience}")

        # è¯¢é—®æ˜¯å¦è¦è¿è¡Œå®Œæ•´æµç¨‹
        print(f"\nâš ï¸  å®Œæ•´æµç¨‹éœ€è¦è°ƒç”¨ LLM APIï¼Œå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´")
        print(f"   æ˜¯å¦ç»§ç»­? (è¿™æ˜¯ä¸€ä¸ªè‡ªåŠ¨æµ‹è¯•ï¼Œå°†è‡ªåŠ¨è·³è¿‡)")

        # è‡ªåŠ¨è·³è¿‡ï¼Œé¿å…æ¶ˆè€— API
        print(f"\nâœ… Pipeline åˆ›å»ºæµ‹è¯•é€šè¿‡")
        print(f"   ğŸ’¡ æç¤º: è¿è¡Œ 'python example_knowledge_path.py' è¿›è¡Œå®Œæ•´æµ‹è¯•")

        return True

    except Exception as e:
        print(f"   âŒ å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def print_summary(results):
    """æ‰“å°æµ‹è¯•æ€»ç»“"""
    print_section("æµ‹è¯•æ€»ç»“")

    total = len(results)
    passed = sum(results.values())

    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"   æ€»è®¡: {total}")
    print(f"   é€šè¿‡: {passed}")
    print(f"   å¤±è´¥: {total - passed}")

    print(f"\nğŸ“‹ è¯¦ç»†ç»“æœ:")
    for i, (test_name, result) in enumerate(results.items(), 1):
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"   {i}. {test_name}: {status}")

    if passed == total:
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼GLM API é›†æˆæ­£å¸¸å·¥ä½œã€‚")
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥:")
        print(f"   1. è¿è¡Œå®Œæ•´ç¤ºä¾‹: python example_knowledge_path.py")
        print(f"   2. æˆ–å¯åŠ¨ API æœåŠ¡å™¨: python api/main.py")
    else:
        print(f"\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")

    return passed == total


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("  GLM (æ™ºè°±) API é›†æˆæµ‹è¯•")
    print("="*70)
    print(f"\nâ° å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    results = {}

    # æµ‹è¯• 1: ç¯å¢ƒå˜é‡é…ç½®
    results["ç¯å¢ƒå˜é‡é…ç½®"] = test_env_config()
    if not results["ç¯å¢ƒå˜é‡é…ç½®"]:
        print("\nâŒ ç¯å¢ƒå˜é‡é…ç½®å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•ã€‚")
        return False

    # æµ‹è¯• 2: LLM å®¢æˆ·ç«¯å¯¼å…¥
    results["LLM å®¢æˆ·ç«¯å¯¼å…¥"] = test_llm_import()
    if not results["LLM å®¢æˆ·ç«¯å¯¼å…¥"]:
        print("\nâŒ LLM å®¢æˆ·ç«¯å¯¼å…¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•ã€‚")
        return False

    # æµ‹è¯• 3: LLM è¿æ¥æµ‹è¯•
    results["LLM è¿æ¥æµ‹è¯•"] = test_llm_connection()
    if not results["LLM è¿æ¥æµ‹è¯•"]:
        print("\nâŒ LLM è¿æ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­æµ‹è¯•ã€‚")
        return False

    # æµ‹è¯• 4: çŸ¥è¯†è·¯å¾„åˆ›å»º
    success, knowledge_path = test_knowledge_path_creation()
    results["çŸ¥è¯†è·¯å¾„åˆ›å»º"] = success

    # æµ‹è¯• 5: å™è¿°åŒ–ä¸Šä¸‹æ–‡ç”Ÿæˆ
    if knowledge_path:
        success, skeleton = test_narrative_generation(knowledge_path)
        results["å™è¿°åŒ–ä¸Šä¸‹æ–‡ç”Ÿæˆ"] = success
    else:
        results["å™è¿°åŒ–ä¸Šä¸‹æ–‡ç”Ÿæˆ"] = False
        skeleton = None

    # æµ‹è¯• 6: Planner Agent
    if skeleton:
        results["Planner Agent"] = test_planner_agent(skeleton)
    else:
        results["Planner Agent"] = False

    # æµ‹è¯• 7: ç®€å•å†…å®¹ç”Ÿæˆ
    results["Pipeline åˆ›å»º"] = test_simple_content_generation()

    # æ‰“å°æ€»ç»“
    all_passed = print_summary(results)

    print(f"\nâ° ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    return all_passed


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
